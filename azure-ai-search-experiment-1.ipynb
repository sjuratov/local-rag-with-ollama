{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisits:\n",
    "- Install [ollama](https://ollama.com/)\n",
    "- Run ollama from terminal: ```ollama run llama3.1```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read environment variables from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "AZURE_OPENAI_EMBEDDING_MODEL: str = os.getenv('AZURE_OPENAI_EMBEDDING_MODEL')\n",
    "AZURE_OPENAI_ENDPOINT: str = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "AZURE_OPENAI_API_KEY: str = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "AZURE_OPENAI_API_VERSION: str = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "\n",
    "AZURE_SEARCH_API_KEY: str = os.getenv('AZURE_SEARCH_API_KEY')\n",
    "AZURE_SEARCH_ENDPOINT: str = os.getenv('AZURE_SEARCH_ENDPOINT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LangChain object to facilitate creation of embeddings using Azure OpenAI embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=AZURE_OPENAI_EMBEDDING_MODEL,\n",
    "    openai_api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LangChain object to facilitate creation of Azure AI Search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "index_name: str = \"langchain-vector-demo\"\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "    azure_search_key=AZURE_SEARCH_API_KEY,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    additional_search_client_options={\"retry_total\": 4}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "\n",
    "docs_list = [item for sublist in docs for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add document chunks to Azure AI Search index (content and embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MTFhZmY1MTUtNTJkYi00ZTJkLWI2MmEtNWY1OWY4OGVmMTU0',\n",
       " 'MDk1ZjQzZTItZDc4MC00ZGE3LTliMzYtZTY0ZDhjZWU1Nzg1',\n",
       " 'ZWYxN2MzZDMtMzYyMy00YjA5LWI3ZTUtZWQ2NTFmYWQ5Zjkx',\n",
       " 'ZGFiM2IzODUtNGIwYi00MDBjLWIxYjctYTM1NTRlYzk2NmQw',\n",
       " 'NDBiZTYwNWUtNjMzMC00YzdiLTliMTgtNTk1MTgxMTkyNTgw',\n",
       " 'ZDM3MDI4ODMtMTZkOS00NGM5LWEzOTgtMzBiMTkyOGE0ZjRk',\n",
       " 'ZDAyZmZmN2UtZTY1Ni00ZmI1LWJjMzktMzA3YzcyYjY2ZTI1',\n",
       " 'OGZiZTJiYzEtZjk2MS00YTk3LTg0MmQtMWYwM2I2OGVmYWZk',\n",
       " 'NzRjZGVjMzQtNjZiNS00ZDkwLWI5MGMtMmQ3YjNkM2Y2NWEw',\n",
       " 'YzY5OWE0NDktZDFhNC00YmRkLWFlYmUtNzE3Y2NhNjczNzg2',\n",
       " 'YzE2ODdlMTYtYTI3ZC00ZGQxLTk0ZjMtZDMwYWQ1NWUyNmEy',\n",
       " 'NzIzMDRkMmYtNjUwMy00ZmIyLWI2ZDgtMGYwNDZkZGY3MTAx',\n",
       " 'YjU1MjQzZDUtY2U2NS00ZWFmLTk4MTQtYzQ4M2EyYzBlMGE3',\n",
       " 'YjQ0Zjc0MzgtNzVjMC00NTdhLWFhNWQtNjUxZWQ3ZWJlNzZl',\n",
       " 'MmUzZDNlOWYtOGYwMC00NDRhLWExMTAtMTMwYjBmYzQ5ODVi',\n",
       " 'MjQyOWYzMDktM2UzYi00MTM2LWIwMDItNjc0ZTI2NjQzYjUw',\n",
       " 'ZDE0MTdjYzUtZDkxZC00Yjg4LTk4ZDgtYzlmMWVkYzBmOTY3',\n",
       " 'NmFjNGFmZjUtMDZhOC00Yzg4LTk3YjgtMDE5YTUzZjUwZDkw',\n",
       " 'NTRkODhkZGQtY2E4Yi00YzhlLWE5ODEtYjAxZDBkNzk5NzA5',\n",
       " 'YjZjOTNmZDktY2ViNy00NDRmLTkxOTctMjMwNzgyMWZkNmRl',\n",
       " 'YWJjZDVlMmEtNmZkMC00ZDA5LWJhYjktMTRiNjIxMDE5OGVh',\n",
       " 'NDIyY2VlN2ItMzNiZS00M2Y3LTg3M2UtZGQ4ZTgxOTQ2MTIz',\n",
       " 'MWNkMjE2ZjEtNmJkNy00NWVlLWFiMDEtMDI1ZjUyMGZiMzZh',\n",
       " 'NmRkNWMwYTEtYzM2Yy00MmNmLWFmY2MtM2RkMTkxZWI3ZDBl',\n",
       " 'NjJjOGEyOWEtN2FiMi00NzU1LTgwOWUtN2NjYjUxZTU5OTAy',\n",
       " 'MzU1YTI1ZTQtYTM0Ni00ZTlmLWIyY2ItNWM4NWYwODM2YTQ0',\n",
       " 'MzI3MDk3NTctNGNlMi00NTU4LThhMzktMjYxNTFmODA4MjAw',\n",
       " 'OTA4NWViMjctMGMxOS00OGZhLTg1YjEtZGQ1Yjk3MTc2MWFm',\n",
       " 'MzNmYmIwN2YtYjAyNC00OWFlLTgxMTUtNTcwYjhjNmY1MmQz',\n",
       " 'OWFlNTEzZmUtZmE2Ni00M2U3LWExY2MtNjBlZWNjYWVhODA1',\n",
       " 'MGYyMGM0Y2QtNDI1Ny00NzY3LWE1NGMtMGI4NTFjMTFhZTA4',\n",
       " 'ZTc4NzA3MDQtNTIzNC00NWIyLWFkNTgtM2JmYjlmNjM2ZTNi',\n",
       " 'NDI1Y2UxZTQtZmMxMy00ZmUyLTgxMDUtYTliZTQzM2VmN2U3',\n",
       " 'MmQzODYyM2QtMTE3MS00YjdlLWEzZjUtYjVhZWQ2MzJhYjI5',\n",
       " 'N2FiODVlMjktOTczZC00MTk2LTk0Y2UtNWM0NjY1NGI5ZThk',\n",
       " 'MzEwN2EyYTctNTMyYy00NmU4LWJiMDUtOTViOTVhN2E0OTJm',\n",
       " 'NjQyNWI2MDgtMWM4Yy00OTc5LThmMjEtMzU5NjlmZGZjODQ5',\n",
       " 'YjQ2YmJlYjAtZDUwMy00ZmQ4LWI1ZTktNmU3YzEzMjk4Nzc2',\n",
       " 'MzVjMTUzYzEtMjMzZS00YWNlLTg3ZGItZjMwMzcwODA3MzMz',\n",
       " 'NzI1N2UxMTctZTE0ZS00NDBhLWI1NGEtYjE5NDNkOGEzNjIw',\n",
       " 'MTRiMGRiODQtNGYxMC00YTUyLWI2NzAtNjk0NjA0ZTZhZmI4',\n",
       " 'NGY1MWU0YWEtODExZi00NjU4LTk3N2YtOTUyYzUzMGQzMzJj',\n",
       " 'MjBkM2M1M2MtZGE4Yy00NmI5LTliMzgtMjkxY2FjYTliNzk2',\n",
       " 'ODM0NWJiODctZTAwZi00NjBhLWJiYTYtZjk3NTc0YjIyZjM2',\n",
       " 'NGRjZDU3NjUtN2VjMi00NTJlLWE5OGMtYzhjMzVjMDUwOTkx',\n",
       " 'ZWM5MzQ5NDQtNjdkNy00NDlmLTk1NzgtNWNjNTc0MmYxNWQ0',\n",
       " 'NTJlNDFiOTUtNjYyMy00NzdmLTgyOWItYjJiNTNlN2M0YTgy',\n",
       " 'ZDZkZWJkNzktNzQ5YS00ZmFmLWEzZjgtYjYyYWIzOGNhZjM3',\n",
       " 'Mjg4NTY3ODYtNzViMS00NTE2LWE3OGYtYzc4ZGMzZWZhNzlh',\n",
       " 'Yjg5YjRmMDYtMmM4ZS00ZjdiLTllYjgtZTVkZjY1ZGI3ZGU0',\n",
       " 'NjAxNTU1ZDMtYTMyNC00NjZjLThlYjItMmQ2ZDUwYjdmMWNj',\n",
       " 'NTRkMmRkYzAtZWRmYi00ODhkLTk0MjAtNTBmMDViYjUzZmYw',\n",
       " 'YjlhNWM4ODItOTFiYS00YmI4LWI3ZTktZGVhNmY4Nzk3Njg2',\n",
       " 'NWU3MGJhZjQtYmEyYy00NjU2LTlkMGItNzg1Yzk3NjQzOTM0',\n",
       " 'Yjg3YmUyNjAtNDczYi00NTRhLThjYmYtMzdkZTk2ZDBkNjJj',\n",
       " 'NWI3ZGFhMGMtNTBlNS00ZDQ3LWJmN2ItNDVlN2JlMWYyNDkw',\n",
       " 'MGVmMmUxZGQtYTA1Yi00ZjZiLWJjMmQtOTM5NWY5MDNjMmVh',\n",
       " 'Nzg3Yzg0Y2ItMjJhZS00NzUyLTljZTktZDFiNjFiM2FkNWQy',\n",
       " 'Yzc1MTg5MjYtYmRlYS00ZTQ3LWEwYjEtOGJjN2E3MzI0MmNl',\n",
       " 'N2ZiYzBhN2ItNzdlYy00N2NjLWE5ZTEtODU2ZjhlNWY2YTg5',\n",
       " 'YTM4NTY4ZWItYjJhMS00NWFkLTgyNzQtZmFmODhlMTViZjZm',\n",
       " 'ZDI0MzYzMDMtODc3OS00YTgwLThlMjItOTk1NmRkZTE3NzRm',\n",
       " 'ZTU0ZTdmOTktNjFmMS00YTQ5LTkwZTctMmJmYWM4ZjMwZmMx',\n",
       " 'MThlZDM4NDgtZDQ0Yi00MzcyLThmOTgtMjA4Zjk2YjVkMDhh',\n",
       " 'NmJiNjQ5MDEtZjg0MS00ZjAzLTkzNGYtZmU2NjAxZjg5Y2Jj',\n",
       " 'Y2ZkNjkxYzQtODIyZS00NDMzLWE5ZWQtMDc2NjZkZWZmMjQ3',\n",
       " 'MDU3NDAxMTQtZTNmYS00ZWU0LWE5MTQtZDFlNmRjZjc4M2Nj',\n",
       " 'YjY3OWUzMWUtM2E3MS00ZjgzLTk4NDAtYTExZDM5NDlmYzA1',\n",
       " 'Y2RmNDRkZWQtZDA5YS00OGJhLTg4OGMtN2RhOTI4NGM3NjVh',\n",
       " 'ZDYyZWRhMmYtNDFmMy00MzYyLTk5OTQtYzIxY2FiMTc3YWIx',\n",
       " 'ODM1M2NiZTYtMTkwNC00ZDQwLWFiNmItZWE4OGYwNTYxNzNi',\n",
       " 'YzE2MDViZWUtMDczYi00MGU4LWE3NWItNjIwMmVkZThlZTUz',\n",
       " 'MDI4MzI0MjctODNhMi00ZTNjLWI2YWItYmIyMTU4OWNkMTky',\n",
       " 'YTI0NzE3MzctOTZiNS00ZGRiLTlmZGUtYmNhYzY4NDUwNTkx',\n",
       " 'N2Q5ZjQ2NDMtNDNhNi00ODUzLWE0ODktYWEzZmIyYWI2MDE4',\n",
       " 'ZTQ2M2QzOWEtOGM4ZC00ZDUxLWI5YmMtZjhjYzBmOTFiMTVh',\n",
       " 'Y2Q3ODYxMWItOTJjYi00NTE5LWJlNDUtMzZlMmI1Y2RhYzAw',\n",
       " 'MzE0ZDAxODktYzBmMS00YzBkLTg4MmUtOTZkZGI0ZTJhN2Nm',\n",
       " 'MTFlZDQ3MWEtNjUxMC00NDk1LWJmZjgtY2RlNjMzOGQwYjBm',\n",
       " 'M2ZlNTZmYmQtMjg4OS00YjA4LWE0ZmQtNzQ5MGFjZThjMDVk',\n",
       " 'NmUwYzFmZTgtMDNhMy00NjdiLTliZGEtODMxYmJiMTA4NGQ1',\n",
       " 'NGQ3MWZhNzQtNDUwNS00MTUxLTk3NzYtMGRlM2MwMDY3MjY3',\n",
       " 'ZGVhMmMwYzQtZGY5MS00YmQ4LTgzZDctNGYwNjAxYTMxNTAx',\n",
       " 'Zjc0ZWE4NjUtZjkxMC00ODdiLThiZGQtYWQyNTM3NTQ0ZWNk',\n",
       " 'YWU4ZTIxZDMtYzZkNi00NjZkLTg5ZDItYmViODgzODgzYzhi',\n",
       " 'N2EyZDUxZDItODMwMi00MmM3LWE3ZDMtNDM1YzY0OTAyN2Iz',\n",
       " 'ZmZhZDA1MjgtMDk2Yy00YTlkLWJjZDYtOWEzZDU5ODQyZTVm',\n",
       " 'ZWFmYjJhZGYtM2Y5MS00MGZhLWI2M2UtYzU5MzFjMTU0MjUx',\n",
       " 'YjY4NmJlODMtNDJmNi00N2EwLWEwNjEtMTUwMzE0YjI1NjUw',\n",
       " 'ZjJjNmVkOGUtOGY3Ni00NzgyLTg3ZTctYTU4NjllZjczZWVm',\n",
       " 'MzM2MGZkNzktNzQ0Zi00ZjNiLWEyOGUtODIwMjUzNjMyZDM0',\n",
       " 'OTA5MzIxOTMtN2Y0Zi00MDNlLWFiZWUtMTQ5NTgwOTczOGI1',\n",
       " 'MDI4ZDFkODMtZjdiYS00MGQzLTkyY2YtYTNiMDI1MjQ1OTVk',\n",
       " 'NjgyYjM2N2ItOTQyMy00YTc2LWFiZTQtMTc4OGJhM2JlMDdi',\n",
       " 'YzFkYzY1NzQtZjM1NC00N2ZjLWJiMDYtNmM5YmVkYWQzMTdi',\n",
       " 'YTg2YTI2N2QtZjRhZi00NjdlLWEzNTQtZmU3Zjk4ZGQ0MzU2',\n",
       " 'MmVmNzljY2MtMTQ1OC00MDY0LThlYzMtNjk2NTJkOTI3NjUw',\n",
       " 'M2RmMjdjMzYtNWM2ZC00ZTE5LTgyMjEtOGNkZmMxMTZmYmYy',\n",
       " 'NGExZWY3MWItYWZjMS00NzdlLThmNGItMjdiMDIzNGMyZjc4',\n",
       " 'MTc4M2VhYzEtOTQ4OC00MWZkLWIxZDAtZjZlN2M2ZDRkMjE0',\n",
       " 'YThmN2Q2ZTgtODM2Yi00YmVlLTlkYTEtNjM1NjJkYmJkOTU3',\n",
       " 'ZWEyYmMyYzQtMTcxNS00YWUzLWJhYTItMzlhZWYxZGZkMzBm',\n",
       " 'NGY2ZDUwYzMtZTI2ZC00YTZhLWExMGUtMGUwMmQzMjU0NjNi',\n",
       " 'NzQzYjViZjUtMzE0OS00MTcxLWIzMjgtZmIxOGNkNDhhYjE1',\n",
       " 'YzJjMDEwM2MtNDM1ZC00OGI2LWI2ODMtMDEwZjdmYmFlZTUx',\n",
       " 'ZmRlM2E1NDEtNWEzNy00ZDNhLWJmMDUtYmRjMzMyNjU4YmVk',\n",
       " 'YWNjNDIxNWYtOWMxMi00YWRhLTllNDktMjQ4MTE5MTM2MzJj',\n",
       " 'NmEwNzM2YTktNzNhMC00Y2QwLWExYzktYWJkZjFjZjk2NmI5',\n",
       " 'MDVkODMwMTEtZmQwZi00MmQzLWEzNWYtYTIzMDI0MzM0Yzli',\n",
       " 'ZWEwYzM5OWEtMTBmYi00OTE2LWI5YjAtMzc1MjgwNjdlYzhl',\n",
       " 'ZGMwZjBhOTgtYTVjMi00ZGZjLTk4N2MtNDZjMDRlMWMwZDVh',\n",
       " 'YzNhMmU1ZGMtNDM2NS00OTIwLWFlZmYtYjlmMjkzNjMwNDI0',\n",
       " 'ZGZkZjk2ZDEtMzEzYy00OGM5LWIyM2UtNTc2NDE1ZTZjMTg5',\n",
       " 'NzA5YjFmNGYtOGZkYy00MGE0LTg1NTEtODc3OWQxYTI3Yjhl',\n",
       " 'M2E2OTM4MzQtZDYyMS00NDA2LWJlODctM2Y1YzcyNGMyMmFm',\n",
       " 'MDc0ODJlZGEtNzYyNC00MWI2LWJkNzgtNTNjYzk4ZTcxYzQ3',\n",
       " 'N2FmMDY3NjEtYWZhYy00OWU2LThlNDYtMTNhOWEwOWVkZTRi',\n",
       " 'NzVjZmNmMTItZDAyMy00YzcwLWI5MTMtODdjZmJjZjIwN2Nj',\n",
       " 'OTEzMzc1NTktMjkxNi00YmU1LWFkY2QtMDI0Y2IxMzc1OTEz',\n",
       " 'ZDI2MjIyNzgtYjY5Yy00ZWFlLTliYjgtYmUxYjRiMjQwNTZj',\n",
       " 'YzBiMzFkZmUtZWIyOS00ODY3LTllYTctMjU0OGYwZGRlYzA5',\n",
       " 'OWJjNDEwMGMtMjFjNC00ZmM3LWJhZjAtM2IzZjJiMmFkYjQ2',\n",
       " 'MDAwNGUzMGUtZmM0ZS00MDhhLWIzZTItYjU0OGVjM2Y1OTZh',\n",
       " 'MzI4Mjk3MWYtYzkwNS00MDc4LTliMzMtMzU4MjI3NGE1NDBm',\n",
       " 'YTZlNmVhYTYtMmQyNi00NzI0LTgxNmYtN2E3MzA5N2VlNTIx',\n",
       " 'ZDc5YjI2NTEtNTk5Ni00MTlmLWI4MzUtZTQ3YzY0ZDdiMmEx',\n",
       " 'ODE4ZDQzNzUtYTBlZi00ZDJmLTkxMDEtMzMxMTQ2OTNmNjJk',\n",
       " 'NTkyY2EzMjktMTU3My00ZWQzLWFhYjgtNDgxYTBlZmFmZjRj',\n",
       " 'MzQ4ZTEyNWItNTJjMy00ODZmLWIyNWEtZjYwMmFlZGI4NmUw',\n",
       " 'YjMxNjIzZGQtYWUwYi00ZDkzLTk5ZDMtODg0ODgxYTA4ZDk5',\n",
       " 'Zjc2ZjQxNGMtMmMwYy00ZGRkLWI0NzctYmUyZjRmZGEyOWJl',\n",
       " 'ZDYxMWVhNzktMmI3My00Y2FlLWI3OTgtYjliNTIwZjI0NDM4',\n",
       " 'OTg1ODhjZDgtN2E2MC00MDE1LTg3YmQtYmE2NDAzOTkyNjdk',\n",
       " 'M2I1OTYwMGQtYjMxYS00NzE4LTk2ZjEtM2FlYjdmNGE4Mzc0',\n",
       " 'YjViNzNjMmYtODMxNi00OWRmLWJjMTctNTM1YzQxNzhkY2Yz',\n",
       " 'ZTRmMGM2NDItMWFhNi00NjFkLWEzNmYtMWViNzYyNTlhMzhi',\n",
       " 'N2E2NjY5ZjktM2Q0NS00NzE5LTkxY2UtODQ2ZmJhZTUyODA1',\n",
       " 'OTFhMTJjYTYtMDdiNi00NjhmLTg2YTEtNmRjNWVkMjFlZjk3',\n",
       " 'MzNhODA0YTAtODFmMi00ZmUwLWJlM2ItNWViOWM1NDg4MDA2',\n",
       " 'MzVkMzQ4ODktYjkwMy00MmY5LTk5NTEtNzA3YzQ5OGU0MDYy',\n",
       " 'YmJjMTc1MTctYTZmOC00ZmNmLTllZjMtMjliNTU3MTMxMmJh',\n",
       " 'ZGE2MDRjZDktMzdiZS00YmNmLWJmMmEtYTk0ZTlmZjdmOGFl',\n",
       " 'OWMxODkyMDEtYWNiMy00OTdmLWIxODYtYzhhM2FkM2NlNzEz',\n",
       " 'Yzk5ZjY1M2UtMDVkZC00ZWE2LTgwNDEtNzQ0ZDExNzFhMzY4',\n",
       " 'Y2Q2YWIxYmItMDk3ZC00N2Q3LTg5ZWYtNWNlNjdlZWRhOTdk',\n",
       " 'M2I2NmMwZTktYjg5MC00ODk4LWI5NzItMzkwNWYzMDhkYWQw',\n",
       " 'OTEwYTVhM2QtY2M4Ny00MzYyLTk4OWEtOTI1MDZjMWJkNWM2',\n",
       " 'OWZmZWE4MzktMzEwYS00NWE4LWJmMWYtNDBkYTJhODdhMDBi',\n",
       " 'ZTgxYTI1OTEtYzBmYS00MDU5LWJjMjUtZDBiMmM4N2E5ZWUw',\n",
       " 'NDVlMzdmODEtNTUyMS00MTkwLWFmNjctZmM0YTQwY2UxM2Iw',\n",
       " 'ZDVkNjY1Y2MtNDYzNy00Mjg2LWE2ODQtNDMzMjlhOGZkYmJk',\n",
       " 'MjA1NjRkODMtMjc4OS00NTI4LTlmY2YtZDIyYTE3NGNhNzI4',\n",
       " 'NGNlNjU1MDYtOGFiOC00ZDAwLTk1ZDAtOGEyMDM4MjAzNTkz',\n",
       " 'NzkwN2NjOTctNDE4Zi00NWVkLTk5ZDctZGE5NGZkODU3MDhm',\n",
       " 'YzU1Zjg3NTYtY2Q2OC00ZWNmLTk3ZGEtOGFkNzJiZWI4YzI4',\n",
       " 'ZGNkM2JiNjMtMGVlNy00Yjc5LWE2ZGMtMzg1YTIwNjU3NmY4',\n",
       " 'YTVkZTUwYjItNTMwZC00YzUwLTgzNWItNDE5NDZiNWViMzIy',\n",
       " 'YjI3MzZkY2UtMjg3ZS00NWE1LWI2YmMtZmUxNjBkNTgxNTEw',\n",
       " 'N2Q4Yjc5ODYtODZhZi00ZGEyLWJhODctMjJkOTk5M2ZiZDRk',\n",
       " 'YmI2MjYxMDUtYjE2NC00YzBkLWFmZDgtYThjYzc2ZjkwNWMy',\n",
       " 'NzJhOWM5MTctOWMxNC00YzczLThkNTgtMmU1MGNjOTYyMDBi',\n",
       " 'MmE3NzRkOTctMmY0ZS00ZTNkLThkZTAtYTYzZWJhMjMxMGQ1',\n",
       " 'Mjg0YzlmYjQtN2IwNy00NTgzLTg2MmQtMDUwMjFlMDVlM2Y4',\n",
       " 'Njc1NmQ2ZjEtMTQzNC00MzgyLWFhYjgtMTQyNWZiYWYzOGYz',\n",
       " 'OWJmZDM1NDUtNDQ2Ni00N2FlLWE1Y2QtOGM2ZWI1NDYyNDM0',\n",
       " 'ODE5NjNjODUtYmE1MS00Mzg2LWE0ZmUtZjQ2ZTE0NGM5NjYz',\n",
       " 'YmIyYTcxODAtYzZjNS00Y2FkLWI3MmYtMDY1ZjY1MGYxYjU1',\n",
       " 'NmU5MDUwODItYTAwMC00MWFhLTk4YzMtYTFkYmRmNzE3ODk4',\n",
       " 'MzcwMjczM2MtMGI4OC00MmQ1LThiNzEtMDVhNjk4NDUxNmIx',\n",
       " 'MzkwYmI0OWQtNmJkNS00MGQ2LThkYjQtNWNjZGEzMmQ0OTM2',\n",
       " 'ODI1ZGMxMDctZmViNC00MWRmLWE2NWUtNDNkN2Q3NDEwYTFj',\n",
       " 'NDgyYWU2ODQtMDhlNi00N2RkLWI0MmMtMWFjZmU3NzZjYzRm',\n",
       " 'NGVmYTE4ZTMtMmFhZi00M2RjLWFiYTAtMGFiOTUwNmE4Yjc0',\n",
       " 'NDlhNTMyODgtODhhZi00MDE5LWFmMTEtNzk3N2NmZmY2ZGQ2',\n",
       " 'YzRhNjI1M2ItM2RjMS00MTQzLTlkMmEtZDhiNDBkNTgzODA5',\n",
       " 'ZTUzYzliNjUtM2Y3OS00MTc0LWFmZGItYmVmMzRiYTE2MGNk',\n",
       " 'YzQ0ZGNhZGEtMTc2NS00MDczLThlMmUtODJkZTQwM2Q3Mzcz',\n",
       " 'Nzg0NjI0YzItYTRhMC00MjFlLTg5ZGEtNTg3NGE3NGRlNzI5',\n",
       " 'MjgyZDE3NmQtMDBlZi00Yzg0LWE1ZTktN2NmN2YxODFiMGQ0',\n",
       " 'NDlkNzJmOWMtOTRmNi00OTI4LWFlNTEtMmU0NmE3MWM3ZTA2',\n",
       " 'Y2EyZDhiZjMtODY0NS00YzQ4LTk0N2YtZTM1Y2YyZWZkMGMz',\n",
       " 'MzE5NTRjY2EtZDViZS00MTAwLWI2NjYtZTk5ODY3NjQwOGIw',\n",
       " 'MjMyMjFhODMtMmQwYy00NDEyLTlkZGQtZjA2ODAxNTk0N2Uy',\n",
       " 'NGZmZmZlOWEtYmQ4My00MGI0LThlODEtNDE2MzViYWJmZjFi',\n",
       " 'OTZlZmQzM2EtOTUxMC00MjZiLTlmMTctYzA1ZTYyMWZlMmE0',\n",
       " 'ODM3OGFiM2ItNDkwZC00NTMxLTliNWUtZTU2MmI3NjJiOTFh',\n",
       " 'NDZlYjI2NTYtYjgwZi00ZDY1LThkNmYtMTBkZTA0YzEzYmYx',\n",
       " 'ZDQ5YWE1NWItMTdmMi00MTdjLTg0OTMtYTYxOWY0Y2QxZGVk',\n",
       " 'YWE3NWU0OTktZmUzZi00Y2EwLWJhNGUtMDA5YjExYjBkNjM5',\n",
       " 'Y2FmNWJhNzYtNjQyOC00NjU5LThlMzgtYWI1NGVhYmM3NWJh',\n",
       " 'MzRmOTliMzktZmU2YS00NTJlLTg4ZjktNDg2MjUzNDI1MzY5',\n",
       " 'ZWY4YjMyMDYtODNhOC00MDE4LTkxNTctNzViM2E0NmE0ZjIy',\n",
       " 'OTNiMWU1NjctMjRiZi00MTBjLWJhNjUtMzkxYzQ1NGEyNjlh',\n",
       " 'NjQ1OTEwOTEtYjk3NC00ZWYxLTgzODUtMmI1ZjFhYTA4ZTJm']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=doc_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search example 1: Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vector_store.similarity_search(\n",
    "    query=\"What is CoT?\",\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Document 1\n",
       "\n",
       "**Source:** https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
       "\n",
       "**Title:** Prompt Engineering | Lil'Log\n",
       "\n",
       "**Description:** Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\n",
       "This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models.\n",
       "\n",
       "**Language:** en\n",
       "\n",
       "**Content:** Two main types of CoT prompting:\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Document 2\n",
       "\n",
       "**Source:** https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
       "\n",
       "**Title:** Prompt Engineering | Lil'Log\n",
       "\n",
       "**Description:** Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\n",
       "This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models.\n",
       "\n",
       "**Language:** en\n",
       "\n",
       "**Content:** Few-shot CoT. It is to prompt the model with a few demonstrations, each containing manually written (or model-generated) high-quality reasoning chains.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Document 3\n",
       "\n",
       "**Source:** https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
       "\n",
       "**Title:** Prompt Engineering | Lil'Log\n",
       "\n",
       "**Description:** Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\n",
       "This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models.\n",
       "\n",
       "**Language:** en\n",
       "\n",
       "**Content:** Fig. 3. Comparing CoT and PoT. (Image source: Chen et al. 2022).\n",
       "External APIs#\n",
       "TALM (Tool Augmented Language Models; Parisi et al. 2022) is a language model augmented with text-to-text API calls. LM is guided to generate |tool-call and tool input text conditioned on task input text to construct API call requests. When |result shows up, the specified tool API is called and the returned result gets appended to the text sequence. The final output is generated following |output token.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "for indx, doc in enumerate(docs, start=1):\n",
    "    md_content = f\"\"\"### Document {indx}\n",
    "\n",
    "**Source:** {doc.metadata['source']}\n",
    "\n",
    "**Title:** {doc.metadata['title']}\n",
    "\n",
    "**Description:** {doc.metadata['description']}\n",
    "\n",
    "**Language:** {doc.metadata['language']}\n",
    "\n",
    "**Content:** {doc.page_content}\n",
    "    \"\"\"\n",
    "    display(Markdown(md_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search example 2: Similarity Search with Relevance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_and_scores = vector_store.similarity_search_with_relevance_scores(\n",
    "    query=\"What is CoT?\",\n",
    "    k=4,\n",
    "    score_threshold=0.80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Document 1\n",
       "\n",
       "**Source:** https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
       "\n",
       "**Title:** Prompt Engineering | Lil'Log\n",
       "\n",
       "**Description:** Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\n",
       "This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models.\n",
       "\n",
       "**Language:** en\n",
       "\n",
       "**Content:** Two main types of CoT prompting:\n",
       "\n",
       "**Score:** 0.85343915\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Document 2\n",
       "\n",
       "**Source:** https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
       "\n",
       "**Title:** Prompt Engineering | Lil'Log\n",
       "\n",
       "**Description:** Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\n",
       "This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models.\n",
       "\n",
       "**Language:** en\n",
       "\n",
       "**Content:** Two main types of CoT prompting:\n",
       "\n",
       "**Score:** 0.85343915\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Document 3\n",
       "\n",
       "**Source:** https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
       "\n",
       "**Title:** Prompt Engineering | Lil'Log\n",
       "\n",
       "**Description:** Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\n",
       "This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models.\n",
       "\n",
       "**Language:** en\n",
       "\n",
       "**Content:** Few-shot CoT. It is to prompt the model with a few demonstrations, each containing manually written (or model-generated) high-quality reasoning chains.\n",
       "\n",
       "**Score:** 0.85109925\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Document 4\n",
       "\n",
       "**Source:** https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
       "\n",
       "**Title:** Prompt Engineering | Lil'Log\n",
       "\n",
       "**Description:** Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\n",
       "This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models.\n",
       "\n",
       "**Language:** en\n",
       "\n",
       "**Content:** Few-shot CoT. It is to prompt the model with a few demonstrations, each containing manually written (or model-generated) high-quality reasoning chains.\n",
       "\n",
       "**Score:** 0.85109925\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for indx, (doc, score) in enumerate(docs_and_scores, start=1):\n",
    "    md_content = f\"\"\"### Document {indx}\n",
    "\n",
    "**Source:** {doc.metadata['source']}\n",
    "\n",
    "**Title:** {doc.metadata['title']}\n",
    "\n",
    "**Description:** {doc.metadata['description']}\n",
    "\n",
    "**Language:** {doc.metadata['language']}\n",
    "\n",
    "**Content:** {doc.page_content}\n",
    "\n",
    "**Score:** {score}\n",
    "    \"\"\"\n",
    "    display(Markdown(md_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LangChain prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks.\n",
    "    Use the following documents to answer the question.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question}\n",
    "    Documents: {documents}\n",
    "    Answer:\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the LLM with Llama 3.1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a chain combining the prompt template and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RAG application class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGApplication:\n",
    "    def __init__(self, retriever, rag_chain):\n",
    "        self.retriever = retriever\n",
    "        self.rag_chain = rag_chain\n",
    "    def run(self, question):\n",
    "        # Retrieve relevant documents\n",
    "        documents = self.retriever.similarity_search_with_relevance_scores(\n",
    "            query=question,\n",
    "            k=4,\n",
    "            score_threshold=0.80,\n",
    "        )\n",
    "        # Extract content from retrieved documents\n",
    "        doc_texts = \"\\\\n\".join([doc[0].page_content for doc in documents])\n",
    "        # Get the answer from the language model\n",
    "        answer = self.rag_chain.invoke({\"question\": question, \"documents\": doc_texts})\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and run the RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is CoT?\n",
      "Answer: CoT stands for Chain-of-Thought, which refers to prompting models with a few demonstrations of high-quality reasoning chains. This allows the model to learn from and replicate the thought process behind the correct answers. It's used in few-shot learning scenarios where the model is given a limited number of examples to learn from.\n"
     ]
    }
   ],
   "source": [
    "rag_application = RAGApplication(vector_store, rag_chain)\n",
    "\n",
    "question = \"What is CoT?\"\n",
    "answer = rag_application.run(question)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
